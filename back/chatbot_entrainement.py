# -*- coding: utf-8 -*-
"""CHATBOT_ENTRAINEMENT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXWjTlLiGlt6nsDKnyTzmo1LEEIMASqd
"""

import pandas as pd

# Charger le fichier CSV
df = pd.read_csv('/content/QUESTIONS_CHATBOT_JURIDIQUE.csv')

# Afficher les premières lignes du fichier CSV pour vérifier
print(df.head())



# Créer une liste contenant chaque question avec son tag (intention)
data = [(row['Question'], row['Tag']) for _, row in df.iterrows()]
# tags = df['Tag'].apply(str).unique()
# Récupérer les tags uniques
tags = df['Tag'].unique()
tags = df['Tag'].apply(str).unique()
print(data[:5])  # Vérifier les premières données
print(tags)  # Vérifier les tags uniques

import spacy
from spacy.training import Example

# Charger un modèle vierge en français
nlp = spacy.blank("fr")

# Ajouter un pipeline de classification textuelle (intentions)
textcat = nlp.add_pipe("textcat")

# Ajouter les étiquettes (tags) pour les intentions
for tag in tags:
    textcat.add_label(tag)



# Préparer les données d'entraînement pour spaCy
TRAIN_DATA = []
for question, tag in data:
    # Créer les annotations pour chaque tag
    cats = {t: 1 if t == tag else 0 for t in tags}

    # Ajouter la question et ses annotations dans les données d'entraînement
    TRAIN_DATA.append((question, {"cats": cats}))

# Afficher quelques exemples des données préparées
print(TRAIN_DATA[:3])



# # Entraînement du modèle avec spaCy
# optimizer = nlp.begin_training()

# # Nombre d'itérations (epochs) pour l'entraînement
# n_iter = 10

# for itn in range(n_iter):
#     losses = {}
#     for text, annotations in TRAIN_DATA:
#         doc = nlp.make_doc(text)
#         example = Example.from_dict(doc, annotations)
#         nlp.update([example], drop=0.2, losses=losses)
#     print(f"Iteration {itn + 1} - Losses: {losses}")

# Entraînement du modèle avec spaCy
optimizer = nlp.begin_training()

# Nombre d'itérations (epochs) pour l'entraînement
n_iter = 10

for itn in range(n_iter):
    losses = {}
    for text, annotations in TRAIN_DATA:
        # Check if 'text' is a string and not nan
        if isinstance(text, str) and text != 'nan':
            doc = nlp.make_doc(text)
            example = Example.from_dict(doc, annotations)
            nlp.update([example], drop=0.2, losses=losses)
    print(f"Iteration {itn + 1} - Losses: {losses}")



# Tester une question utilisateur
test_question = "Quels sont mes droits en cas de licenciement ?"
doc = nlp(test_question)

# Afficher les prédictions d'intention
print(doc.cats)  # Renvoie un dictionnaire avec les probabilités pour chaque intention
predicted_tag = max(doc.cats, key=doc.cats.get)  # Trouver l'intention avec la plus haute probabilité
predicted_tag



# Fonction pour récupérer la réponse à partir de l'intention prédite
def get_reponse(question):
    # Prédire l'intention
    doc = nlp(question)
    predicted_tag = max(doc.cats, key=doc.cats.get)  # Trouver l'intention avec la plus haute probabilité

    # Rechercher la réponse correspondante dans le DataFrame (df doit être défini avec ton fichier CSV)
    reponse = df[df['Tag'] == predicted_tag].iloc[0]['Réponse']

    return reponse

# Tester la fonction
question_utilisateur = "Quels documents dois-je fournir pour une plainte ?"
reponse = get_reponse(question_utilisateur)
print(f"Réponse : {reponse}")

# Tester plusieurs questions
questions = [
    "Comment déposer une plainte ?",
    "Quel est le coût pour déposer une plainte ?",
    "Quels sont mes droits en cas de licenciement ?"
]

for question in questions:
    reponse = get_reponse(question)
    print(f"Question : {question}")
    print(f"Réponse : {reponse}")
    print("-" * 50)





import pandas as pd
import spacy

# Charger le fichier CSV contenant les questions, réponses, et tags
df = pd.read_csv('/content/QUESTIONS_CHATBOT_JURIDIQUE.csv')

# Supprimer les lignes où 'Question' est manquant
df = df.dropna(subset=['Question'])

# Convertir toutes les questions en chaînes de caractères
df['Question'] = df['Question'].apply(str)
df['Tag'] = df['Tag'].apply(str)

# # Charger le modèle spaCy entraîné
# nlp = spacy.load("/content/modele_chatbot_juridique")

import pandas as pd
import spacy

# Charger le fichier CSV contenant les questions, réponses, et tags
df = pd.read_csv('/content/QUESTIONS_CHATBOT_JURIDIQUE.csv')

# Supprimer les lignes où 'Question' est manquant
df = df.dropna(subset=['Question'])

# Convertir toutes les questions en chaînes de caractères
df['Question'] = df['Question'].apply(str)
df['Tag'] = df['Tag'].apply(str)

# Charger le modèle spaCy entraîné
# nlp = spacy.load("/content/modele_chatbot_juridique")

# Fonction pour calculer la similarité de texte entre deux questions
def calculer_similarite(question1, question2):
    doc1 = nlp(question1)
    doc2 = nlp(question2)
    return doc1.similarity(doc2)

# Fonction pour obtenir la meilleure réponse en fonction du tag et de la similarité
def get_meilleure_reponse(question, seuil_similarite=0.6):
    # Étape 1 : Prédire l'intention (tag) avec spaCy
    doc = nlp(question)
    predicted_tag = max(doc.cats, key=doc.cats.get)  # Le tag prédit avec la probabilité la plus élevée

    # Étape 2 : Récupérer toutes les questions ayant ce tag
    questions_tag = df[df['Tag'] == predicted_tag]

    # Si aucune question n'est trouvée pour ce tag, retourner un message d'erreur
    if questions_tag.empty:
        return "Je ne comprends pas votre question, pouvez-vous la reformuler ?"

    # Étape 3 : Calculer la similarité entre la question utilisateur et chaque question sous ce tag
    meilleure_similarite = 0
    meilleure_question = None
    meilleure_reponse = None

    for _, row in questions_tag.iterrows():
        similarite = calculer_similarite(question, row['Question'])

        # Trouver la question avec la meilleure similarité
        if similarite > meilleure_similarite:
            meilleure_similarite = similarite
            meilleure_question = row['Question']
            meilleure_reponse = row['Réponse']

    # Si la similarité est inférieure au seuil, retourner la première réponse disponible pour ce tag
    if meilleure_similarite < seuil_similarite:
        print(f"Similarité trop faible ({meilleure_similarite:.2f}), retournons la première réponse disponible.")
        return questions_tag.iloc[0]['Réponse']  # Retourner la première réponse du tag

    return meilleure_reponse

# Tester avec plusieurs questions utilisateur
questions = [
    "Où puis-je déposer une plainte ?",
    "Quel est le coût pour déposer une plainte ?",
    "Quels documents dois-je fournir pour déposer une plainte ?",
    "Je veux savoir combien de temps j'ai pour porter plainte",  # Question pas très claire
    "Est-ce que je peux porter plainte anonymement ?"
]

for question in questions:
    reponse = get_meilleure_reponse(question)
    print(f"Question : {question}")
    print(f"Réponse : {reponse}")
    print("-" * 50)









# # Sauvegarder le modèle entraîné
# nlp.to_disk("/content/modele_chatbot_juridique")

# # Télécharger le modèle sur ton local depuis Colab
# from google.colab import files
# import shutil

# shutil.make_archive("modele_chatbot_juridique", 'zip', "/content/modele_chatbot_juridique")
# files.download("modele_chatbot_juridique.zip")

